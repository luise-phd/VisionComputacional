{"cells":[{"cell_type":"markdown","metadata":{"id":"nFIZyVqD_Dyu"},"source":["## Cargando redes pre-entrenadas\n","Las redes preentrenadas son modelos que ya han sido entrenados con grandes conjuntos de datos, como ImageNet, y pueden ser reutilizados para resolver tareas similares sin necesidad de entrenarlos desde cero. En Keras, puedes cargar fácilmente arquitecturas populares como ResNet, VGG, MobileNet o Inception, lo que permite aprovechar su capacidad de generalización y ahorrar tiempo computacional.\n","\n","Puedes explorar las opciones disponibles en el siguiente enlace: https://keras.io/applications/"]},{"cell_type":"markdown","source":["## Elaborado por: Luis Eduardo Ordoñez"],"metadata":{"id":"EQSoHgAoR7WC"}},{"cell_type":"markdown","source":["### Importar librerías"],"metadata":{"id":"FjpqtKDHS6q6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuYkROQg_Dyw"},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","from google.colab.patches import cv2_imshow\n","\n","# Importa el modelo ResNet50 preentrenado desde la biblioteca Keras Applications\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","# Importa utilidades para cargar y procesar imágenes\n","from tensorflow.keras.preprocessing import image\n","# Importa funciones específicas de ResNet50 para preprocesar imágenes y decodificar predicciones\n","from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions"]},{"cell_type":"markdown","source":["### ResNet50\n","Es una arquitectura de red neuronal convolucional profunda compuesta por 50 capas, diseñada para tareas de clasificación de imágenes. Fue propuesta por Microsoft Research y es parte de la familia de redes Residual Networks (ResNet), cuya principal innovación es el uso de \"conexiones residuales\" o atajos que permiten entrenar redes muy profundas sin sufrir problemas de degradación del aprendizaje. Gracias a estas conexiones, ResNet50 puede aprender representaciones visuales complejas de forma más eficiente, superando en precisión a modelos anteriores como VGG. Entrenado en el conjunto de datos ImageNet, ResNet50 es ampliamente utilizado para transfer learning y aplicaciones prácticas en visión computacional, como reconocimiento de objetos, detección de rostros y análisis de imágenes médicas."],"metadata":{"id":"HQaxjdwCShZ1"}},{"cell_type":"code","source":["# Carga el modelo ResNet50 con pesos preentrenados en ImageNet\n","# Este modelo puede reconocer miles de clases visuales comunes\n","model = ResNet50()"],"metadata":{"id":"o-MhGXVQTAQQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Conectar Google Colab con Google Drive"],"metadata":{"id":"WiUshm_aP6XN"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"mkHCkfQCP5rZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cargar imagen"],"metadata":{"id":"aNl1KOXdUk_G"}},{"cell_type":"code","source":["# Editar la ruta local en Google Drive de la imagen que se va a visualizar\n","imagen = cv2.imread('/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/bufalo.jpg')\n","\n","# Obtener dimensiones originales\n","alto, ancho = imagen.shape[:2]\n","\n","# Definir nuevo ancho deseado\n","nuevo_ancho = 400\n","\n","# Calcular nuevo alto proporcional\n","proporcion = nuevo_ancho / ancho\n","nuevo_alto = int(alto * proporcion)\n","\n","# Redimensionar con proporción conservada\n","imagen_redim = cv2.resize(imagen, (nuevo_ancho, nuevo_alto))\n","\n","# Mostrar imagen\n","cv2_imshow(imagen_redim)"],"metadata":{"id":"C6QSf0_tUnqY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Predicción"],"metadata":{"id":"KYBeuLAmZD3Z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXCwDdgI_Dyx"},"outputs":[],"source":["# Editar la ruta local en Google Drive de la imagen que se va a clasificar\n","img_path = '/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/bufalo.jpg'\n","\n","# Carga la imagen desde la ruta y la redimensiona al tamaño requerido por ResNet50 (224x224 píxeles)\n","img = image.load_img(img_path, target_size=(224, 224))\n","\n","# Convierte la imagen a un arreglo NumPy (matriz de píxeles)\n","x = image.img_to_array(img)\n","\n","# Expande las dimensiones del arreglo para simular un lote (batch) de una sola imagen\n","x = np.expand_dims(x, axis=0)\n","\n","# Aplica el preprocesamiento específico requerido por ResNet50 (reescala y normaliza la imagen)\n","x = preprocess_input(x)\n","\n","# Realiza la predicción utilizando el modelo ResNet50\n","preds = model.predict(x)\n","\n","# Muestra las predicciones decodificadas (nombre de clase e índice de confianza)\n","print('Predicciones:')\n","for n in decode_predictions(preds)[0]:\n","    print(n[1:])  # Muestra solo el nombre de la clase y la probabilidad (sin el ID de clase)"]},{"cell_type":"markdown","source":["### Cargar imagen"],"metadata":{"id":"v5qBf9lbVW2a"}},{"cell_type":"code","source":["imagen = cv2.imread('/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/elephant.jpg')\n","alto, ancho = imagen.shape[:2]\n","nuevo_ancho = 400\n","proporcion = nuevo_ancho / ancho\n","nuevo_alto = int(alto * proporcion)\n","imagen_redim = cv2.resize(imagen, (nuevo_ancho, nuevo_alto))\n","cv2_imshow(imagen_redim)"],"metadata":{"id":"Q1cGyFVGVZNz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Predicción"],"metadata":{"id":"F3a0glkhZHFE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8Mnt6RV_Dyx"},"outputs":[],"source":["img_path = '/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/elephant.jpg'\n","\n","img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","preds = model.predict(x)\n","print('Predicciones:', )\n","for n in decode_predictions(preds)[0]:\n","    print(n[1:])"]},{"cell_type":"markdown","source":["### Cargar imagen"],"metadata":{"id":"b135OI5jVvXC"}},{"cell_type":"code","source":["imagen = cv2.imread('/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/jabali.jpg')\n","alto, ancho = imagen.shape[:2]\n","nuevo_ancho = 400\n","proporcion = nuevo_ancho / ancho\n","nuevo_alto = int(alto * proporcion)\n","imagen_redim = cv2.resize(imagen, (nuevo_ancho, nuevo_alto))\n","cv2_imshow(imagen_redim)"],"metadata":{"id":"R6Psba5JVxXh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Predicción"],"metadata":{"id":"8__F8W9EZIob"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2M4a-oKY_Dyy"},"outputs":[],"source":["img_path = '/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/jabali.jpg'\n","\n","img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","preds = model.predict(x)\n","print('Predicciones:', )\n","for n in decode_predictions(preds)[0]:\n","    print(n[1:])"]},{"cell_type":"markdown","source":["### Cargar imagen"],"metadata":{"id":"rvWSD0ueWFD3"}},{"cell_type":"code","source":["imagen = cv2.imread('/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/pirarucu.jpg')\n","alto, ancho = imagen.shape[:2]\n","nuevo_ancho = 400\n","proporcion = nuevo_ancho / ancho\n","nuevo_alto = int(alto * proporcion)\n","imagen_redim = cv2.resize(imagen, (nuevo_ancho, nuevo_alto))\n","cv2_imshow(imagen_redim)"],"metadata":{"id":"h45crRCoV_14"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Predicción"],"metadata":{"id":"Ry3-Lb7yZKTz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Lpdc7BI_Dyz"},"outputs":[],"source":["img_path = '/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/pirarucu.jpg'\n","\n","img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","preds = model.predict(x)\n","print('Predicciones:', )\n","for n in decode_predictions(preds)[0]:\n","    print(n[1:])"]},{"cell_type":"markdown","source":["### Cargar imagen"],"metadata":{"id":"6JvqvK36WO2L"}},{"cell_type":"code","source":["imagen = cv2.imread('/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/tilapia.jfif')\n","alto, ancho = imagen.shape[:2]\n","nuevo_ancho = 400\n","proporcion = nuevo_ancho / ancho\n","nuevo_alto = int(alto * proporcion)\n","imagen_redim = cv2.resize(imagen, (nuevo_ancho, nuevo_alto))\n","cv2_imshow(imagen_redim)"],"metadata":{"id":"oUVa1hAdWLZF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Predicción"],"metadata":{"id":"SZ5IrNzgZMtp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mw7v0p_-_Dy0"},"outputs":[],"source":["img_path = '/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/tilapia.jfif'\n","\n","img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","preds = model.predict(x)\n","print('Predicciones:', )\n","for n in decode_predictions(preds)[0]:\n","    print(n[1:])"]},{"cell_type":"markdown","source":["### Cargar imagen"],"metadata":{"id":"L8OUkM_9WMqu"}},{"cell_type":"code","source":["imagen = cv2.imread('/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/landscape.jpg')\n","alto, ancho = imagen.shape[:2]\n","nuevo_ancho = 400\n","proporcion = nuevo_ancho / ancho\n","nuevo_alto = int(alto * proporcion)\n","imagen_redim = cv2.resize(imagen, (nuevo_ancho, nuevo_alto))\n","cv2_imshow(imagen_redim)"],"metadata":{"id":"Ua0QxiJJWWXW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Predicción"],"metadata":{"id":"mYQ6S0LBZN7d"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeDgZjhY_Dy1"},"outputs":[],"source":["img_path = '/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/landscape.jpg'\n","\n","img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","preds = model.predict(x)\n","print('Predicciones:', )\n","for n in decode_predictions(preds)[0]:\n","    print(n[1:])"]},{"cell_type":"markdown","source":["### Cargar imagen"],"metadata":{"id":"6guTTQzKXRk3"}},{"cell_type":"code","source":["imagen = cv2.imread('/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/arbol.png')\n","alto, ancho = imagen.shape[:2]\n","nuevo_ancho = 200\n","proporcion = nuevo_ancho / ancho\n","nuevo_alto = int(alto * proporcion)\n","imagen_redim = cv2.resize(imagen, (nuevo_ancho, nuevo_alto))\n","cv2_imshow(imagen_redim)"],"metadata":{"id":"uUB8qgmAXR4v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Predicción"],"metadata":{"id":"T62tK_QVZPi7"}},{"cell_type":"code","source":["img_path = '/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/arbol.png'\n","\n","img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","preds = model.predict(x)\n","print('Predicciones:', )\n","for n in decode_predictions(preds)[0]:\n","    print(n[1:])"],"metadata":{"id":"fsFKIJ84YQdn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cargar imagen"],"metadata":{"id":"WZDA2s0zZSzW"}},{"cell_type":"code","source":["imagen = cv2.imread('/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/laptop.jpg')\n","alto, ancho = imagen.shape[:2]\n","nuevo_ancho = 250\n","proporcion = nuevo_ancho / ancho\n","nuevo_alto = int(alto * proporcion)\n","imagen_redim = cv2.resize(imagen, (nuevo_ancho, nuevo_alto))\n","cv2_imshow(imagen_redim)"],"metadata":{"id":"fZXjq_kOYftq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Predicción"],"metadata":{"id":"u-otn-KjZVIm"}},{"cell_type":"code","source":["img_path = '/content/drive/MyDrive/Corhuila/Visión Computacional/Notebooks/imgs/laptop.jpg'\n","\n","img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","preds = model.predict(x)\n","print('Predicciones:', )\n","for n in decode_predictions(preds)[0]:\n","    print(n[1:])"],"metadata":{"id":"CMhEgc1SXSDi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fq90yBDG_Dy1"},"source":["## Otras redes de detección de objetos en  tiempo real:\n","Clasificación de imágenes con localización\n","https://pjreddie.com/darknet/yolo/"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}